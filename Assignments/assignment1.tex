% Created 2024-03-10 Sun 19:19
% Intended LaTeX compiler: pdflatex
\RequirePackage{rotating}
\documentclass[12pt]{amsart}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{wasysym}
\newcommand{\Cov}{\ensuremath{\mbox{Cov}}}
\renewcommand{\Pr}{\ensuremath{\mbox{Pr}}}
\newcommand{\Eq}[1]{(\ref{eq:#1})}
\usepackage{bm}\usepackage{econometrics}
\usepackage{breqn}
\newcommand{\T}{\top}
\newtheorem{proposition}{Proposition} \newcommand{\Prop}[1]{Proposition \ref{prop:#1}}
%\newtheorem{problem}{Problem} \newcommand{\Prob}[1]{Problem \ref{prob:#1}}
%\newtheorem{theorem}{Theorem} \newcommand{\Thm}[1]{Theorem \ref{thm:#1}}
%\newtheorem{corollary}{Corollary} \newcommand{\Cor}[1]{Corollary \ref{cor:#1}}
%\newtheorem{remark}{Remark} \newcommand{\Rem}[1]{Remark \ref{rem:#1}}
%\newtheorem{condition}{Condition} \newcommand{\Cond}[1]{Condition \ref{cond:#1}}
%\newtheorem{lemma}{Lemma} \newcommand{\Lem}[1]{Lemma \ref{lem:#1}}
%\newtheorem{assumption}{Assumption} \newcommand{\Ass}[1]{Assumption \ref{ass:#1}}
\newcommand{\Fig}[1]{Figure \ref{fig:#1}} \newcommand{\Tab}[1]{Table \ref{tab:#1}}
\usepackage{dsfont}\newcommand{\one}{\ensuremath{\mathds{1}}}
\usepackage{xcolor}
\newcommand{\rv}[1]{\ensuremath{\textcolor{red}{#1}{}}}
%\newcommand{\rv}[1]{\ensuremath{{}_{rv}{#1}{}}}
%\newcommand{\rv}[1]{\ensuremath{\underline{#1}{}}}
\newcommand{\rvy}{\rv{y}}
\newcommand{\rvX}{\rv{X}}
\newcommand{\rvx}{\rv{x}}
\newcommand{\rvu}{\rv{u}}
\newcommand{\do}[1]{\ensuremath{\mbox{do}(#1)}}
\renewcommand{\E}{\ensuremath{\mathds{E}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\author{Ethan Ligon}
\date{Due March 22, 2024}
\title{Assignment 1}
\hypersetup{
 pdfauthor={Ethan Ligon},
 pdftitle={Assignment 1},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.1 (Org mode 9.7)}, 
 pdflang={English}}
\usepackage[style=apa]{biblatex}
\addbibresource{/home/ligon/bibtex/main.bib}
\begin{document}

\maketitle
You should complete all exercises.   You are strongly encouraged to work as a  team, and to turn in a single assignment for grading.   The principal deliverable you turn in should be a pdf.
\section{Admin}
\label{sec:org5b3867c}
Some steps to help us share code via \texttt{git}:
\begin{enumerate}
\item Your team should ``fork'' the repository \url{https://github.com/ligonteaching/ARE212\_Materials}, and each  team member should have ``write'' access to the repo.
\item When submitting code for assignments, please provide links to that code in the pdf.
\end{enumerate}
\section{Exercises}
\label{sec:org1f74800}
\begin{enumerate}
\item From ARE210, recall (Section 9 in Mahajan's ``Handout 1'') the rule for
computing the distribution of certain transformations of random
variables (The ``inverse Jacobian rule'').

Let \((\rvx,\rvy)\) be independently distributed continuous random
variables possessing densities \(f_x\) and \(f_y\).  Let \(\rv{z} = \rvx +
      \rvy\).  Use the rule to obtain an expression for the distribution
of \(\rv{z}\).

\item We've discussed ways to program a convolution of random
variables in a Jupiter notebook [\href{https://github.com/ligonteaching/ARE212\_Materials/blob/master/random\_variables0.ipynb)}{ipynb}] [\href{http://datahub.berkeley.edu/user-redirect/interact?account=ligonteaching\&repo=ARE212\_Materials\&branch=master\&path=random\_variables0.ipynb}{datahub}].  As in the
notebook, consider a discrete random variable \(s\) and a continuous random variable \(x\).
Prove that the convolution of \(\rv{s}\) and \(\rvx\) (or, informally,
\(\rvx+\rv{s}\)) has a continuous distribution, as suggested by the figure at
the end of the notebook, \textbf{or} establish that the figure is wrong or
misleading.

\item Let \(\mA\) be an \(m\times n\) matrix.  A matrix \(\mA^-\) is a
\emph{generalized inverse} of \(\mA\) if \(\mA\mA^-\mA=\mA\).  Such a generalized
inverse can be shown to always exist.  If \(\mA\) is a matrix
of zeros, what can we say about \(\mA^-\)?

\item Econometricians spend a great deal of time writing down
linear regressions relating an object ``Why'' to an object ``Ex'', but
sometimes use quite distinct notations to express this
regression.  Following our discussion in class, suggest a
notation for each of the three following cases:

\begin{enumerate}
\item ``Why'' is a scalar random variable, while ``Ex'' is a vector
random variable;

\item ``Why'' is a single \emph{realization} of a scalar random variable,
while ``Ex'' is similarly a single \emph{realization};

\item ``Why'' is a \emph{vector} of \(N\) realizations, while ``Ex'' is
similarly a \emph{matrix} of realizations.
\end{enumerate}

\item Let \(\mA\) be an \(m\times n\) matrix.
\end{enumerate}

\begin{center}
\fbox{
\begin{minipage}[c]{.6\linewidth}
Moore-Penrose Inverse

\rule[.8em]{\linewidth}{2pt}

A matrix \(\mA^+\) is a ``Moore-Penrose'' generalized inverse if:

\begin{itemize}
\item \(\mA\mA^+\mA = \mA\);
\item \(\mA^+\mA\mA^+ = \mA^+\);
\item \(\mA^+\mA\) is symmetric; and
\item \(\mA\mA^+\) is symmetric.
\end{itemize}
\end{minipage}
}
\end{center}

\begin{center}
\fbox{
\begin{minipage}[c]{.6\linewidth}
Full rank factorization

\rule[.8em]{\linewidth}{2pt}

Let \(\mA\) be an \(n\times m\) matrix of rank \(r\).  If \(\mA = \mL\mR\), where \(\mL\) is a      \(n\times r\) full column rank matrix, and \(\mR\) is a \(r\times m\) full row rank matrix, then \(\mL\mR\) is a \emph{full rank factorization} of \(\mA\).
\end{minipage}
}
\end{center}
\begin{center}
\fbox{
\begin{minipage}[c]{.6\linewidth}
Fact

\rule[.8em]{\linewidth}{2pt}

Provided only that \(r>0\), the Moore-Penrose inverse \(\mA^+ = \mR^{\T}(\mL^{\T}\mA\mR^{\T})^{-1}\mL^{\T}\) exists and is unique.
\end{minipage}
}
\end{center}
\begin{enumerate}
\item If \(\mA\) is a matrix of zeros, what is \(\mA^+\)?

\item Show  that if \(\mX\) has full column rank, then \(\mX^+=(\mX^\T\mX)^{-1}\mX^\T\) (this is sometimes called the ``left inverse), and \(\mX^+\mX=\mI\).
\item Use the result of (2) to solve for \(\vb\) in the (matrix) form of the regression  \(\vy = \mX b + \vu\) if \(\mX^{\T}\vu=0\).
\end{enumerate}
\section{Convolutions}
\label{sec:org29a4159}
We've discussed ways to program a convolution of random variables in a
Jupyter notebook [\href{https://github.com/ligonteaching/ARE212\_Materials/blob/master/random\_variables0.ipynb)}{ipynb}] [\href{http://datahub.berkeley.edu/user-redirect/interact?account=ligonteaching\&repo=ARE212\_Materials\&branch=master\&path=random\_variables0.ipynb}{datahub}].

\begin{enumerate}
\item As in the notebook, consider a discrete random variable \(s\) and
a continuous random variable \(x\).  Prove that the convolution
of \(s\) and \(x\) (or, informally, \(x+s\)) has a continuous
distribution, as suggested by the figure at the end of the
notebook, \textbf{or} establish that the figure is wrong or misleading.

\item The notebook develops a simple class
\texttt{ConvolvedContinuousAndDiscrete} to allow for the creation and
manipulations of (you guessed it) convolutions of a continuous rv
with a discrete rv.  Can you develop a similar class for
convolutions of independent discrete random variables?

\item Same as (2), but convolutions of independent continuous random variables?
\end{enumerate}
\section{General Weighted Linear Regressions}
\label{sec:orga6f2885}
List the main regression estimators you encountered in the first half of the class.  For each estimator, establish whether it belongs to the class of general weighted linear regressions
\begin{equation}
   T'Y = T'X\beta + T'u.
\end{equation}
For the estimator you've listed, if it is a general weighted linear regression then what is the form of \(T\)?  Is \(T\) random?  If the estimator is \emph{not} in this class, show why.
\section{Simultaneous Equations}
\label{sec:orge3e9cef}
When we defined the general weighted regression, we didn't assume
anything about the \textbf{dimension} of the different objects except that
they were 'conformable.'

So: consider
\begin{equation}
    y = X\beta + u,\qquad\text{with $\mbox{E}T'u = 0$,}
\end{equation}
and where \(y=[y_1,y_2,\dots,y_k]\), so that if you had a sample of \(N\)
observations realizations of \(y\) would be an \(N\times k\) matrix.

\begin{enumerate}
\item What does our assumption of conformability then imply about the
dimensions of \(X\), \(\beta\), \(T\), and \(u\)?
\item Could you use the estimator we developed in
\href{http://datahub.berkeley.edu/user-redirect/interact?account=ligonteaching\&repo=ARE212\_Materials\&branch=master\&path=weighted\_regression.ipynb}{weighted\_regression.ipynb} to estimate this system of
simultaneous equations?
\item Extend the code in \href{http://datahub.berkeley.edu/user-redirect/interact?account=ligonteaching\&repo=ARE212\_Materials\&branch=master\&path=weighted\_regression.ipynb}{weighted\_regression.ipynb} to actually
estimate \(\beta\) in the case with \(k=3\).
\item What additional assumptions are necessary to estimate the
distribution of the estimator of \(\beta\)?
\end{enumerate}
\section{SUR}
\label{sec:org6d026f8}
Picking up from the discussion of simultaneous equations above, where
\(y\) is \(N\times k\), and
\begin{equation}
     y = X\beta + u.
\end{equation}

If \(X\) is \(N\times\ell\) and \(\mbox{cov}(u|X) = \Omega\) then this
is a generalization of the assumption of homoskedasticity to a
multivariate setting; the resulting structure is called a system of
“Seemingly Unrelated Regressions” (SUR).

\begin{enumerate}
\item If \(\Omega\) isn't diagonal then there's a sense in which the
different equations in the system are dependent, since observing a
realization of, say, \(y_1\) may change our prediction of \(y_2\).
(This is why the system is called ``seemingly'' unrelated.) Describe
this dependence formally.
\item Adapt the code in
\href{http://datahub.berkeley.edu/user-redirect/interact?account=ligonteaching\&repo=ARE212\_Materials\&branch=master\&path=weighted\_regression.ipynb}{weighted\_regression.ipynb}
so that the data-generating process for \(u\) can accommodate a
general covariance matrix such as \(\Omega\), and let \(X=T\). Estimate \(\beta\).
\item How are the estimates obtained from this SUR system different from
what one would obtain if one estimated equation by equation using
OLS?
\end{enumerate}
\section{Food expenditures in India}
\label{sec:org24af5ae}
The NSS surveys in India pioneered (in considerable part due to
Mahalanobis) a wide variety of methodological innovations in
sampling, questionnaire design, and have been
among the most ambitious regularly collected data on
household behavior and characteristics until recently.

The most recently publicly released data on household expenditures
was the ``68th round'', collected in 2011--12.  (More recent data has
been collected, but suppressed for political reasons.)  Data on
household-level total food (and a few other non-durable)
expenditures from the 68th round is available \href{https://drive.google.com/drive/folders/1z2-XcRHuCpsIBbYRvGe3nxga8gCKICVA?usp=sharing}{here}, in the file
\texttt{total\_expenditures.parquet}.  (You can use the
\texttt{pandas.read\_parquet} method to read these files--you may need to
install some additional dependencies such as \texttt{pyarrow}.).

\begin{enumerate}
\item Use these data to produce a figure describing
the distribution of non-durable expenditures across households,
measured in INR, using a Gaussian kernel and some bandwidth \(h\).  What
are the strengths and weaknesses of your figure in terms of what
it conveys about the underlying distribution?  Can weaknesses be
addressed by choosing a different bandwidth or kernel?  (Nothing
formal required here---I encourage you to simply play around.)
\item Once you've arrived at some favorite kernel \& bandwidth (say
$$\hat{f}^h(x)$$) describing the density of expenditures, can you
use the ``inverse Jacobian'' rule to describe instead the density
of \emph{log} expenditures?  Write code to produce this figure.
\item Instead of the route you've taken in (2), choose some kernel \&
bandwidth to estimate the density of log expenditures directly.
How do the approaches of (2) and (3) compare?
\end{enumerate}
\section{``Plug-in'' Kernel Bias Estimator}
\label{sec:org7685f79}

In our discussion of bias of the kernel density estimator in lecture we
constructed an ``Oracle'' estimator, which can be implemented when we
know the true density $$f$$ that we're trying to estimate.

Of course, the Oracle estimator is only feasible when we don't need it.  What
about the idea of using the same expression for bias as in the Oracle
case, but replacing \(f\) with our estimate \(\hat{f}\)?  Would this tell
us anything useful?  If so, under what conditions?  What pitfalls
might one encounter?
\end{document}
