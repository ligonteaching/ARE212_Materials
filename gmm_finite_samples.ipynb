{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Finite Sample Properties of Linear GMM\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Introduction\n\n"]},{"cell_type":"markdown","metadata":{},"source":["GMM provides a generalized way to think about instrumental\nvariables estimators, and we also have evidence that the finite\nsample properties of these estimators may be poor.  Here we&rsquo;ll\nconstruct a simple Monte Carlo framework within which to evaluate\nthe finite-sample behavior of GMM linear IV estimators.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Asymptotic Variance of GMM estimator\n\n"]},{"cell_type":"markdown","metadata":{},"source":["If we have $\\mbox{E}g_j(\\beta)g_j(\\beta)^\\top=\\Omega$ and\n$\\mbox{E}\\frac{\\partial g_j}{\\partial b^\\top}(\\beta)=Q$ then we&rsquo;ve\nseen that the asymptotic variance of the optimally weighted GMM\nestimator is\n$$\n       V_b = \\left(Q^\\top\\Omega^{-1}Q\\right)^{-1}.\n   $$\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Data Generating Process\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We consider the linear IV model\n\n\\begin{align*}\n   y &= X\\beta + u\\\\\n   \\mbox{E}Z^\\top u &= 0\n\\end{align*}\n\nThus, we need to describe processes that generate $(X,Z,u)$.\n\nThe following code block defines the important parameters governing\nthe DGP; this is the &ldquo;TRUTH&rdquo; we&rsquo;re designing tools to reveal.  \n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\nfrom numpy.linalg import inv\n\n## Play with us!\nbeta = 1     # \"Coefficient of interest\"\ngamma = 1    # Governs effect of u on X\nsigma_u = 1  # Note assumption of homoskedasticity\n## Play with us!\n\n# Let Z have order ell, and X order 1, with Var([X,Z]|u)=VXZ\n\nell = 4 # Play with me too!\n\n# Arbitrary (but deterministic) choice for VXZ\nA = np.sqrt(1/np.arange(1,(ell+1)**2+1)).reshape((ell+1,ell+1)) \n\n\n## Below here we're less playful.\n\n# Var([X,Z]|u) is constructed so that pos. def.\nVXZ = A.T@A \n\nQ = VXZ[1:,[0]]  # EZX'\n\ntruth = (beta,gamma,sigma_u,VXZ)\n\n## But play with Omega if you want to introduce heteroskedascity\nOmega = (sigma_u**2)*VXZ[1:,1:] # E(Zu)(u'Z')\n\n# Asymptotic variance of optimally weighted GMM estimator:\nprint(inv(Q.T@inv(Omega)@Q))"]},{"cell_type":"markdown","metadata":{},"source":["Now code to generate $N$ realizations of $(y,X,Z)$ given some &ldquo;truth&rdquo;\n$(beta,gamma,sigma_u,VXZ)$:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.stats import distributions as iid\n\ndef dgp(N,beta,gamma,sigma_u,VXZ):\n    \"\"\"Generate a tuple of (y,X,Z).\n\n    Satisfies model:\n        y = X@beta + u\n        E Z'u = 0\n        Var(u) = sigma^2\n        Cov(X,u) = gamma*sigma_u^2\n        Var([X,Z}|u) = VXZ\n        u,X,Z mean zero, Gaussian\n\n    Each element of the tuple is an array of N observations.\n\n    Inputs include\n    - beta :: the coefficient of interest\n    - gamma :: linear effect of disturbance on X\n    - sigma_u :: Variance of disturbance\n    - VXZ :: Cov([X,Z|u])\n    \"\"\"\n    \n    u = iid.norm.rvs(size=(N,1))*sigma_u\n\n    # \"Square root\" of VXZ via eigendecomposition\n    lbda,v = np.linalg.eig(VXZ)\n    SXZ = v@np.diag(np.sqrt(lbda))\n\n    # Generate normal random variates [X*,Z]\n    XZ = iid.norm.rvs(size=(N,VXZ.shape[0]))@SXZ.T\n\n    # But X is endogenous...\n    X = XZ[:,[0]] + gamma*u\n    Z = XZ[:,1:]\n\n    # Calculate y\n    y = X*beta + u\n\n    return y,X,Z"]},{"cell_type":"markdown","metadata":{},"source":["Check on DGP:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["N = 1000\n\ndata = dgp(N,*truth)\n\ny,X,Z = data # Unpack tuple to check on things\n\n# Check that we've computed things correctly:\nprint(VXZ)\n\nprint(np.cov(np.c_[X,Z].T) - VXZ)"]},{"cell_type":"markdown","metadata":{},"source":["### Estimation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Now that we have a data-generating process we proceed with under\n   the conceit that we can observe samples generated by this process,\n   but otherwise temporarily &ldquo;forget&rdquo; the properties of the DGP, and use the\n   generated data to try to reconstruct aspects of the DGP.\n\nIn our example, we consider using the optimally weighted linear IV\nestimator, and define a function which computes observation-level\ndeviations from expectations for this model. To testimate a different\nmodel this is the function we&rsquo;d want to re-define.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def gj(b,y,X,Z):\n    \"\"\"Observations of g_j(b).\n\n    This defines the deviations from the predictions of our model; i.e.,\n    e_j = Z_ju_j, where EZ_ju_j=0.\n\n    Can replace this function to testimate a different model.\n    \"\"\"\n    return Z*(y - X*b)"]},{"cell_type":"markdown","metadata":{},"source":["#### Construct sample moments\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Begin by defining a function to construct the sample moments given\n    the data and a parameter estimate $b$:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def gN(b,data):\n    \"\"\"Averages of g_j(b).\n\n    This is generic for data, to be passed to gj.\n    \"\"\"\n    e = gj(b,*data)\n\n    # Check to see more obs. than moments.\n    assert e.shape[0] > e.shape[1]\n    \n    return e.mean(axis=0)"]},{"cell_type":"markdown","metadata":{},"source":["#### Define estimator of Egg'\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Next we define a function to compute covariance matrix of moments.\nRe-centering can be important in finite samples, even if irrelevant in\nthe limit.  Since we have $\\mbox{E}g_j(\\beta)=0$ under the null we may\nas well use this information when constructing our weighting matrix.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def Omegahat(b,data):\n    e = gj(b,*data)\n\n    # Recenter! We have Eu=0 under null.\n    # Important to use this information.\n    e = e - e.mean(axis=0) \n    \n    return e.T@e/e.shape[0]"]},{"cell_type":"markdown","metadata":{},"source":["Next, check construction of criterion given our data.  We want\nsomething that looks nice and quadratic, at least in the neighborhood\nof $\\beta$.\n\nCheck construction of weighting matrix for our data at true parameter $\\beta$.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["Winv = Omegahat(beta,data) \nprint(Winv)"]},{"cell_type":"markdown","metadata":{},"source":["Define the criterion function given a weighting matrix $W$:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def J(b,W,data):\n\n    m = gN(b,data) # Sample moments @ b\n    N = data[0].shape[0]\n\n    return N*m.T@W@m # Scale by sample size"]},{"cell_type":"markdown","metadata":{},"source":["Next, check construction of criterion given our data.  We want\nsomething that looks nice and quadratic, at least in the neighborhood\nof $\\beta$.  Note that comparing the criterion to the critical values\nof the $\\chi^2$ statistic gives us an alternative way to construct\nconfidence intervals.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from matplotlib import pyplot as plt\n%matplotlib inline\n\n# Limiting distribution of criterion (under null)\nlimiting_J = iid.chi2(ell-1)\n\n# Limiting SE of b\nsigma_0 = np.sqrt(inv(Q.T@inv(Winv)@Q)/N)[0][0] \n\n# Choose 8 sigma_0 neighborhood of \"truth\"\nB = np.linspace(beta-4*sigma_0,beta+4*sigma_0,100)\nW = inv(Winv)\n\n_ = plt.plot(B,[J(b,W,data) for b in B.tolist()])\n_ = plt.axhline(limiting_J.isf(0.05),color='r')"]},{"cell_type":"markdown","metadata":{},"source":["#### Two Step Estimator\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We next implement the two-step GMM estimator\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.optimize import minimize_scalar\n\ndef two_step_gmm(data):\n\n    # First step uses identity weighting matrix\n    W1 = np.eye(gj(1,*data).shape[1])\n\n    b1 = minimize_scalar(lambda b: J(b,W1,data)).x \n\n    # Construct 2nd step weighting matrix using\n    # first step estimate of beta\n    W2 = inv(Omegahat(b1,data))\n\n    return minimize_scalar(lambda b: J(b,W2,data))"]},{"cell_type":"markdown","metadata":{},"source":["Now let&rsquo;s try it with an actual sample, just to see that things work:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["soltn = two_step_gmm(data)\n\nprint(\"b=%f, J=%f, Critical J=%f\" % (soltn.x,soltn.fun,limiting_J.isf(0.05)))"]},{"cell_type":"markdown","metadata":{},"source":["### Monte Carlo Experiment\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Now our experiment begins.  We set our frequentist hats firmly on our\nheads, and draw repeated samples of data, each generating a\ncorresponding estimate of beta.  Then the empirical distribution of\nthese samples tells us about the *finite* sample performance of our estimator.\n\nWe&rsquo;ll generate a sample of estimates of $b$ by drawing repeated\nsamples of size $N$:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["N = 1000 # Sample size\n\nD = 1000 # Monte Carlo draws\n\nb_draws = []\nJ_draws = []\nfor d in range(D):\n    soltn = two_step_gmm(dgp(N,*truth))\n    b_draws.append(soltn.x)\n    J_draws.append(soltn.fun)\n\n_ = plt.hist(b_draws,bins=int(np.ceil(np.sqrt(N))))\n_ = plt.axvline(beta,color='r')"]},{"cell_type":"markdown","metadata":{},"source":["### Distribution of Monte Carlo draws vs. Asymptotic distribution\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Compare Monte Carlo standard errors with asymptotic approximation:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Limiting distribution of estimator\n\nlimiting_b = iid.norm(scale=sigma_0)\n\nprint(\"Bootstrapped standard errors: %g\" % np.std(b_draws))\nprint(\"Asymptotic approximation: %g\" % sigma_0)\nprint(\"Critical value for J statistic: %g (5%%)\" % limiting_J.isf(.05))"]},{"cell_type":"markdown","metadata":{},"source":["Now construct probability plot (bootstrapped $b$s vs. quantiles of\nlimiting distribution):\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.stats import probplot\n\n_ = probplot(b_draws,dist=limiting_b,fit=False,plot=plt)"]},{"cell_type":"markdown","metadata":{},"source":["Next, consider the a $p$-$p$ plot for $J$ statistics (recall these\nshould be distributed $\\chi^2_{\\ell-1}$).\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def ppplot(data,dist):\n    data = np.array(data)\n\n    # Theoretical CDF, evaluated at points of data\n    P = [dist.cdf(x) for x in data.tolist()]\n\n    # Empirical CDF, evaluated at points of data\n    Phat = [(data<x).mean() for x in data.tolist()]\n\n    fig, ax = plt.subplots()\n    \n    ax.scatter(P,Phat)\n    ax.plot([0,1],[0,1],color='r') # Plot 45\n    ax.set_xlabel('Theoretical Distribution')\n    ax.set_ylabel('Empirical Distribution')\n    ax.set_title('p-p Plot')\n\n    return ax\n    \n_ = ppplot(J_draws, limiting_J)"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}