{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM Estimation of Logit Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a &ldquo;logit&rdquo; regression; score function from MLE is:\n",
    "$$\n",
    "     \\frac{1}{N}\\sum_j X_j\\left(y_j - \\frac{e^{X_j\\beta}}{1+e^{X_j\\beta}}\\right) = 0.\n",
    "  $$\n",
    "Let $u_j = y_j - \\frac{e^{X_j\\beta}}{1+e^{X_j\\beta}}$; if we have some $Z$ such that\n",
    "$\\mbox{E}(u|Z) = 0$ then we can construct further moment conditions\n",
    "$$\n",
    "     \\frac{1}{N}\\sum_j Z_j\\left(y_j - \\frac{e^{X_j\\beta}}{1+e^{X_j\\beta}}\\right) = 0.\n",
    "  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM Estimator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Some different options for moments...\n",
    "\n",
    "def mle(b,y,X,Z):\n",
    "    \"\"\"Observations of score for MLE estimator.\n",
    "\n",
    "    Moment condition is E(y_j - p(x_jb))x_j = 0,\n",
    "    where p(xb) = e^{xb}/(1+e^{xb})\n",
    "    \"\"\"\n",
    "    p = np.exp(X*b)  # This is actually the odds\n",
    "    p = p/(1+p)      # This is probability y=1\n",
    "\n",
    "    return X*(y - p)\n",
    "\n",
    "def nonlinear_iv(b,y,X,Z):\n",
    "    \"\"\"Observations for restriction that Z\n",
    "    orthogonal to score.\n",
    "\n",
    "    Moment condition is E(Z_jy_j - Z_jexp(x_jb)) = 0\n",
    "    \"\"\"\n",
    "    p = np.exp(X*b)  # This is actually the odds\n",
    "    p = p/(1+p)      # This is probability y=1\n",
    "\n",
    "    return Z*(y - p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generating Process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import distributions as iid\n",
    "\n",
    "def dgp(N,beta,VXZ,gamma=1):\n",
    "    \"\"\"Generate a tuple of (y,X,Z).\n",
    "\n",
    "    Satisfies model:\n",
    "        Pr(y=1|X) = f(X@beta,gamma)\n",
    "        u = y - f(X@beta,gamma)\n",
    "        E(u|Z) = 0\n",
    "        f(x,gamma) = (exp(x)/(1+exp(x)))**gamma\n",
    "        Var([X,Z}) = VXZ\n",
    "        X,Z mean zero, Gaussian\n",
    "\n",
    "    Each element of the tuple is an array of N observations.\n",
    "    When gamma=1 this reduces to the logit model\n",
    "\n",
    "    Inputs include\n",
    "    - beta :: Governs effect of X on probability y=1\n",
    "    - gamma :: Governs curvature of function\n",
    "    - VXZ :: Var([X,Z])\n",
    "    \"\"\"\n",
    "    \n",
    "    # \"Square root\" of VXZ via eigendecomposition\n",
    "    lbda,v = np.linalg.eig(VXZ)\n",
    "    SXZ = v@np.diag(np.sqrt(lbda))\n",
    "\n",
    "    # Generate normal random variates [X*,Z]\n",
    "    XZ = iid.norm.rvs(size=(N,VXZ.shape[0]))@SXZ.T\n",
    "\n",
    "    X = XZ[:,[0]] \n",
    "    Z = XZ[:,1:]\n",
    "\n",
    "    # Calculate y\n",
    "    pi = np.exp(X*beta)\n",
    "    pi = (pi/(1+pi))**gamma\n",
    "\n",
    "    y = iid.bernoulli(pi).rvs(size=(N,1))\n",
    "\n",
    "    return y,X,Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Truth (Mark I)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version of the truth the form of the distribution of $y$ is\n",
    "known up to the parameter $\\beta$; MLE takes advantage of this.\n",
    "\n",
    "Choose some parameters to establish the &ldquo;truth&rdquo;:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "## Play with us!\n",
    "beta = 2     # \"Coefficient of interest\"\n",
    "\n",
    "## Play with us!\n",
    "\n",
    "# Let Z have order ell, and X order 1, with Var([X,Z]|u)=VXZ\n",
    "\n",
    "ell = 1 # Play with me too!\n",
    "\n",
    "# Arbitrary (but deterministic) choice for VXZ\n",
    "A = np.sqrt(1/np.arange(1,(ell+1)**2+1)).reshape((ell+1,ell+1)) \n",
    "\n",
    "## Below here we're less playful.\n",
    "\n",
    "# Var([X,Z]|u) is constructed so that pos. def.\n",
    "VXZ = A.T@A \n",
    "\n",
    "truth = (beta,VXZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Analysis of MLE via GMM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we take the score to be our moment condition; $Z$ doesn&rsquo;t appear,\n",
    "so estimator is just identified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b=0.282076, J=0.000000, Critical J=nan\n"
     ]
    }
   ],
   "source": [
    "import gmm # Just code defined last class\n",
    "\n",
    "gmm.gj = mle  # Redefine gj function to use MLE score\n",
    "\n",
    "data = dgp(10000,*truth)\n",
    "\n",
    "soltn = gmm.one_step_gmm(data)\n",
    "\n",
    "limiting_J = iid.chi2(0)\n",
    "\n",
    "print(\"b=%f, J=%f, Critical J=%f\" % (soltn[0],soltn[1],limiting_J.isf(0.05)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our experiment begins.  We set our frequentist hats firmly on our\n",
    "heads, and draw repeated samples of data, each generating a\n",
    "corresponding estimate of beta.  Then the empirical distribution of\n",
    "these samples tells us about the *finite* sample performance of our estimator.\n",
    "\n",
    "We&rsquo;ll generate a sample of estimates of $b$ by drawing repeated\n",
    "samples of size $N$, until estimates of the covariance of our\n",
    "estimates converge:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gmm # Just code defined last class\n",
    "\n",
    "N = 1000 # Sample size\n",
    "\n",
    "tol = 1e-3\n",
    "\n",
    "b_mle = []\n",
    "b_nliv = []\n",
    "J_nliv = []\n",
    "\n",
    "d=0\n",
    "oldV = 0\n",
    "newV = 1\n",
    "while d<30 or np.linalg.norm(oldV-newV) > tol:\n",
    "    d += 1\n",
    "    oldV = newV\n",
    "    data = dgp(N,*truth)\n",
    "    gmm.gj = mle\n",
    "    soltn_mle = gmm.two_step_gmm(data)\n",
    "    b_mle.append(soltn_mle.x)\n",
    "\n",
    "    gmm.gj = nonlinear_iv\n",
    "    soltn_nliv = gmm.two_step_gmm(data)\n",
    "    b_nliv.append(soltn_nliv.x)\n",
    "    J_nliv.append(soltn_nliv.fun)\n",
    "\n",
    "    newV = np.var(b_nliv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare MLE & NLIV estimates:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "_ = plt.scatter(b_mle,b_nliv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
