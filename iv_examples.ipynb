{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Instrumental Variables in Canonical Demand & Supply Model\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Data-Generating Process\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\nimport pandas as pd\nfrom scipy.stats import distributions as iid\n\n# Unobservable component of supply shock z\n# Can have any distribution one pleases\nw = iid.beta(1,2,loc=-iid.beta(1,2).mean()) # Centered for convenience\n\n# Structural parameters;\n(alpha,beta) = (-1,2)     \nsigma = {'u':1/2,'v':1/3}\nmu = {'u':2,'v':-1}\n\n# u,v assumed independent\nu = iid.norm(loc=mu['u'], scale=sigma['u'])  # Demand shocks\nv = iid.norm(loc=mu['v'], scale=sigma['v'])  # Supply shocks\n\n# Reduced form coefficients\npi = [[-beta/(alpha - beta), -1/(alpha - beta)],\n     [ alpha/(alpha - beta), 1/(alpha - beta)]]\n\n# Generate N realizations of system\n# Outcomes have columns (p,q,z)\ndef wright_dgp(N):\n    \"\"\"\n    Generate data consistent with Wright (1934) hog demand and supply.\n\n    Returns a pandas dataframe with N observations on (p,q,z), where\n    z is understood to be a supply shock.\n    \"\"\"\n    \n    # Arrange shocks into an Nx2 matrix\n    U = np.c_[u.rvs(N), v.rvs(N)]\n\n    # Matrix product gives [q,p]; label by putting into df\n    df = pd.DataFrame(U@pi,columns=['q','p'])\n\n    Udf = pd.DataFrame(U,columns=['u','v']) # For future reference\n\n    # Relate v and z (need not be linear)\n    unobserved_shock = w.rvs(N)/10\n    df['z'] = (1-unobserved_shock)*np.exp(4*Udf['v'] - unobserved_shock)\n    df['Constant'] = 1\n\n    # Include a constant term in both X & Z\n    return df[['q']],df[['Constant','p']],df[['Constant','z']]"]},{"cell_type":"markdown","metadata":{},"source":["### Estimation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Let&rsquo;s write some code to estimate the parameters of the regression\n   model using the estimator devised above (the &ldquo;simple IV estimator&rdquo;):\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n\ndef draw_b(N,dgp):\n    \"\"\"\n    Generate a random variate $b$ from a sample of $N$ draws from a function dgp.\n    \"\"\"\n    y,X,Z =  dgp(N)\n\n    b = pd.Series(np.linalg.solve(Z.T@X,Z.T@y.squeeze()),index=X.columns) # Solve normal eqs\n\n    u = y.squeeze() - (X@b).squeeze()\n    sigma2 = np.var(u)\n\n    avarb = sigma2*(X.T@Z)@np.linalg.inv(Z.T@Z)@(Z.T@X).values/N\n\n    ase = pd.Series(np.sqrt(np.diag(avarb)),index=X.columns)\n\n    return b,ase\n\nb,ase = draw_b(10000,wright_dgp)\n\nprint(f\"b=\\n{b.T}\")\nprint()\nprint(f\"sigma(b)=\\n{ase}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Inference\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Now consider the point that the estimator $b$ is a random variable.\n Under the assumptions of the *model* a Central Limit Theorem applies,\n so it&rsquo;s asymptotically normal, with\n\n\\begin{multline}\n     \\mbox{avar}(b) = \\E[(X^\\T Z)(Z^\\T Z)^{-1}(Z^\\T X)]^{-1} (X^\\T Z)(Z^\\T Z)^{-1}uu^\\T(Z^\\T Z)^{-1}(Z^\\T X) \\\\\n\\cdot[(Z^\\T X)(Z^\\T Z)^{-1}(Z^\\T Z)]^{-1},\n \\end{multline}\n\nwhich in the homoskedastic case simplifies to\n$$\n     \\mbox{avar}(b) = \\E[(X^\\T Z)(Z^\\T Z)^{-1}(Z^\\T X)]^{-1}\\sigma^2.\n $$\nBut in any finite sample the just\n identified linear IV estimator can be feisty.  Let&rsquo;s explore using a\n little Monte Carlo experiment.  Let&rsquo;s begin by constructing a\n slightly more transparent data-generating process, in which $Z$ and\n $X$ have a linear relationship:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.stats import distributions as iid\n\ndef linear_dgp(N,beta,gamma,pi,sigma_u,sigma_v):\n    u = iid.norm(scale=sigma_u).rvs(N)\n    v = iid.norm(scale=sigma_v).rvs(N)\n    Z = iid.norm().rvs(N)\n\n    X = Z*pi + v\n    y = X*beta + u\n\n    df = pd.DataFrame({'y':y,'x':X,'z':Z,'Constant':1})\n\n    return df[['y']],df[['Constant','x']],df[['Constant','z']]"]},{"cell_type":"markdown","metadata":{},"source":["The next bit of code *repeatedly* draws new random samples and\n  calculates $b$ from them; we then construct a histogram of the\n  resulting estimates.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from matplotlib import pyplot as plt\n\nB = pd.DataFrame([draw_b(100,lambda N: linear_dgp(N,1,0,.01,1,1))[0]['x'] for i in range(1000)])\nB.hist(bins=int(np.ceil(np.sqrt(B.shape[0]))))"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Consider the $p-p$ plot of the empirical distribution of our estimates $b$ against the normal distribution:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def ppplot(data,dist):\n    data = np.array(data)\n\n    # Theoretical CDF, evaluated at points of data\n    P = [dist.cdf(x) for x in data.tolist()]\n\n    # Empirical CDF, evaluated at points of data\n    Phat = [(data<x).mean() for x in data.tolist()]\n\n    fig, ax = plt.subplots()\n    \n    ax.scatter(P,Phat)\n    ax.plot([0,1],[0,1],color='r') # Plot 45\n    ax.set_xlabel('Theoretical Distribution')\n    ax.set_ylabel('Empirical Distribution')\n    ax.set_title('p-p Plot')\n\n    return ax\n\nppplot(B,iid.norm(loc=B.mean(),scale=ase['p']))"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}
