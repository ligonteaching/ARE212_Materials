{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Kernel Density Estimation:PROPERTIES:\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### The Wiggly Distribution\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We address the following problem.  There exists a random variable\nwith a density $f$.  We call this distribution the &ldquo;wiggly&rdquo;\ndistribution, and use the magic of `scipy.stats` to define it as follows:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.stats import rv_continuous\nimport numpy as np\n\nclass wiggly_gen(rv_continuous):\n\n    \"\"\"Wiggly distribution\"\"\"\n\n    def _pdf(self, x):\n        d = self.b - self.a\n        return (np.sin(x*6*2*np.pi/d) + 1)/(self.b - self.a)\n\nwiggly = wiggly_gen(a=0.,b=2*np.pi,name='wiggly')\n\nx = wiggly()\n\nf = x.pdf  # Name of true pdf, for pedagogical convenience"]},{"cell_type":"markdown","metadata":{},"source":["Plot the pdf:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_pdf(x,npts=1000,ax=None):\n\n    if ax is None:\n        fig,ax = plt.subplots()\n\n    V = np.linspace(x.a,x.b,npts)\n    ax.plot(V,[x.pdf(v) for v in V])\n\n    return ax\n\nax = plot_pdf(x)"]},{"cell_type":"markdown","metadata":{},"source":["### Sample\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Now, suppose we don&rsquo;t know the distribution of the random variable\n$x$, but we can draw a sample of realizations, which we&rsquo;ll put in a `pandas.Series`.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n\nn = 1000 # Sample size\n\nS = pd.Series(x.rvs(size=n))"]},{"cell_type":"markdown","metadata":{},"source":["### Estimation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["How can we use this random sample to estimate the density?\nOne simple idea is to look at the histogram (scaled to sum to one):\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["S.hist(bins=int(np.sqrt(len(S))),density=True)"]},{"cell_type":"markdown","metadata":{},"source":["This captures some of the important features of the distribution, but\nlet&rsquo;s see if we can do better by using kernel density tools.\n\nStart by defining a kernel:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["sqrt3 = np.sqrt(3)  # Avoid repeated evaluation of this for speed...\n\nk = lambda u: (np.abs(u) < sqrt3)/(2*sqrt3)  # Rectangular kernel\n\n# k = lambda u: np.exp(-(u**2)/2)/np.sqrt(2*np.pi) # Gaussian kernel"]},{"cell_type":"markdown","metadata":{},"source":["Now define the kernel estimator:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def kernel_estimator(X,h):\n    \"\"\"\n    Use data X to estimate a density, using bandwidth h.\n    \"\"\"\n    return lambda x: k((X-x)/h).mean()/h"]},{"cell_type":"markdown","metadata":{},"source":["We already have a random sample: let&rsquo;s try using it to estimate $f$:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["fhat = kernel_estimator(S,0.05) \n\nfhat(1)  # Try to evaluate at a point"]},{"cell_type":"markdown","metadata":{},"source":["Now graph our estimate\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["ax = plot_pdf(x) # \"True\" distribution\n\nV = np.linspace(0,2*np.pi,1000)\n\nax.plot(V,[fhat(x) for x in V])"]},{"cell_type":"markdown","metadata":{},"source":["Play with other bandwidths, and compare with &ldquo;truth&rdquo;\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["ax = plot_pdf(x) # \"True\" distribution\n\nfhat = kernel_estimator(S,0.25) \n\nax.plot(V,[fhat(x) for x in V])"]},{"cell_type":"markdown","metadata":{},"source":["Consider the Silverman rule of thumb:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["h_silverman = S.std()*S.count()**(-1/5)*1.06\n\nax = plot_pdf(x) # \"True\" distribution\n\nfhat = kernel_estimator(S,h_silverman) \n\nax.plot(V,[fhat(x) for x in V])"]},{"cell_type":"markdown","metadata":{},"source":["### Bias\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Hansen (Probability, &sect; 17.4) develops an expression for the bias of\nthe  kernel density estimator:\n$$\n  \\mbox{Bias}(x) = \\int k(u)\\left(f(x+hu)-f(x)\\right)du.\n$$\nThus, bias depends only on the kernel, the bandwidth, and the\n(unknown!) density $f$.\n\nHere we use an &ldquo;oracle&rdquo; estimator to estimate the bias, in which we\nmake use of the fact that (for the experiment in this notebook) we in\nfact *do* do know $f$.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Oracle Bias Estimator\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.integrate import quad # General purpose integration\n\nintegrand = lambda u,x,h: k(u)*(f(x + h*u) - f(x))\n\n# bias will return a pair (integral,absolute error tolerance),\n# where the latter is a promise that the absolute error \n# from numerical integration is smaller than the reported tolerance.\nbias = lambda x,h: quad(lambda u: integrand(u,x,h), # Hold x & h fixed in integral\n                        a=0, b=2*np.pi)   # Limits of integration"]},{"cell_type":"markdown","metadata":{},"source":["Try evaluating this:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["bias(x=1,h=0.1)"]},{"cell_type":"markdown","metadata":{},"source":["Try plotting!\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["ax = plot_pdf(x)\nh = 0.1\nax.plot(V,[bias(v,h=h) for v in V])"]},{"cell_type":"markdown","metadata":{},"source":["Compare with actual estimate:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["ax = plot_pdf(x)\n\nh = 0.1\n\nfhat = kernel_estimator(S,h=h) \n\nax.plot(V,[bias(v,h=h) for v in V])\n\nax.plot(V,[fhat(v) for v in V])"]},{"cell_type":"markdown","metadata":{},"source":["### Variance\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The finite-sample *variance* of the kernel estimator (so now sampling\nvariation matters) can be computed as just the variance of the sample\n&ldquo;smooths&rdquo; $k(x_i - x)/h$, so\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def Vhat(X,h):\n    \"\"\"\n    Use data X to estimate the variance of $\\hat{f}$ as a function of $x$.\n    \"\"\"\n    n = X.count()\n    return lambda x: k((X-x)/h).var()/(n*h**2)"]},{"cell_type":"markdown","metadata":{},"source":["Try evaluating this:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["vhat = Vhat(S,h=0.1)"]},{"cell_type":"markdown","metadata":{},"source":["Try plotting!\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["plt.plot(V,[vhat(v) for v in V])"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}