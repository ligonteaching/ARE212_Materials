{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Regression in `python`                                   :jupyter:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that $T$ and $u$ are &ldquo;independent&rdquo; (or at least\n",
    "orthogonal) variables means that if we want to compute a\n",
    "&ldquo;classical&rdquo; regression we&rsquo;d do it something like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define independent random variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "k = 3 # Number of observables in T\n",
    "\n",
    "mu = [0]*k\n",
    "Sigma=[[1,0.5,0],\n",
    "       [0.5,2,0],\n",
    "       [0,0,3]]\n",
    "\n",
    "T = multivariate_normal(mu,Sigma)\n",
    "\n",
    "u = multivariate_normal(cov=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define `X`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $X$ can depend on $T$ and $u$.  This dependence needn&rsquo;t be\n",
    "linear!  For example, suppose $X=T^3D + u$, where $D$ is an\n",
    "$\\ell\\times k$ matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construct Sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct a sample of observables $(y,X,T)$ we just use the regression equation,\n",
    "      plus an assumption about the value of $\\beta$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = [1/2,1]\n",
    "\n",
    "D = np.random.random(size=(3,2)) # Generate random 3x2 matrix\n",
    "\n",
    "N=1000 # Sample size\n",
    "\n",
    "# Now: Transform rvs into a sample\n",
    "T = T.rvs(N)\n",
    "\n",
    "u = u.rvs(N) # Replace u with a sample\n",
    "\n",
    "X = (T**3)@D  # Note use of ** operator for exponentiation\n",
    "\n",
    "y = X@beta + u # Note use of @ operator for matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Turn to estimation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we now have data on *realizations* $(y,X,T)$.  Now forget\n",
    "     that we know $\\beta$ and let&rsquo;s estimate it, using weighted least\n",
    "     squares.  As a numerical matter it&rsquo;s better to avoid explicitly\n",
    "     inverting the $(T^T X)$ matrix; instead we can solve the &ldquo;normal&rdquo;\n",
    "     equations\n",
    "\n",
    "\\begin{align*}\n",
    "   T'y &= T' X b + T' u\\\\\n",
    "   \\mbox{E}(T'u) = 0\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numerical solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the classical case we were trying to solve a linear system that\n",
    " took the form $Ab=0$, with $A$ a square matrix.  In the present case\n",
    " we&rsquo;re also trying to solve a linear system, but with a matrix $A$\n",
    " that may have more rows than columns.  Provided the rows are linearly\n",
    " independent, this implies that we have an **overidentified** system of\n",
    " equations.  We&rsquo;ll return to the implications of this later, but for\n",
    " now this also calls for a different numerical approach, using\n",
    " `np.linalg.lstsq` instead of `np.linalg.solve`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import inv, sqrtm\n",
    "\n",
    "b = np.linalg.lstsq(T.T@X,T.T@y,rcond=None)[0] # lstsq returns several results\n",
    "\n",
    "e = y - X@b\n",
    "\n",
    "print(b)\n",
    "\n",
    "TXplus = np.linalg.pinv(T.T@X) # Moore-Penrose pseudo-inverse\n",
    "\n",
    "# Covariance matrix of b\n",
    "vb = e.var()*TXplus@T.T@T@TXplus.T  # u is known to be homoskedastic\n",
    "\n",
    "print(vb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerical solution to the weighted regression problem involves computing `lstsq(T.T@X,T.T@y)`, i.e., the regression of $y^*=T'y$ on $X^*=T'X$.  How many observations are there in $(X^*,y^*)$?    How would you interpret this regression?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
